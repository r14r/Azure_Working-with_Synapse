{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Access Synapse SQL table from Synapse Spark\n",
        "\n",
        "This notebook provides examples of how to read data from Synapse SQL into a Spark context and how to write the output of Spark jobs into an Synapse SQL table.\n",
        "\n",
        "\n",
        "## Limits\n",
        "- Scala is the only supported language by the Spark-SQL connector.\n",
        "- The Spark connector can only read colummns without space in its header in the sql pool.\n",
        "- Columns with time definition in the sql pool not yet supported.\n",
        "- You need to define a container on the workspace's primary or linked storage as the temp data folder.\n",
        "\n",
        "## Pre-requisites\n",
        "You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
        "\n",
        "    \n",
        "    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Register the DataFrame as a SQL temporary view: source"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark \n",
        "# Load sample data from azure open dataset in pyspark\n",
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()\n",
        "\n",
        "print('Register the DataFrame as a SQL temporary view: source')\n",
        "hol_df.createOrReplaceTempView('source')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "holiday_nodate: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|countryOrRegion|holidayName              |normalizeHolidayName     |isPaidTimeOff|countryRegionCode|\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|Czech          |Den české státnosti      |Den české státnosti      |null         |CZ               |\n|Norway         |Søndag                   |Søndag                   |null         |NO               |\n|Sweden         |Söndag                   |Söndag                   |null         |SE               |\n|India          |Gandhi Jayanti           |Gandhi Jayanti           |true         |IN               |\n|Germany        |Tag der Deutschen Einheit|Tag der Deutschen Einheit|null         |DE               |\n+---------------+-------------------------+-------------------------+-------------+-----------------+\nonly showing top 5 rows"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Remove datetime from the data source\n",
        "val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\n",
        "holiday_nodate.show(5,truncate = false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a Spark dataframe into your sql pool\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.SqlAnalyticsConnector._\nimport com.microsoft.spark.sqlanalytics.utils.Constants\nsql_pool_name: String = mysqlpool\naccount_name: String = ruxune\ntemp_folder: String = data"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Write the dataframe into your sql pool\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name\n",
        "\n",
        "holiday_nodate.write\n",
        "    .sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new **dbo.PublicHoliday** table show up there."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read from a SQL Pool table with Spark\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "spark_read: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|countryOrRegion|holidayName              |normalizeHolidayName     |isPaidTimeOff|countryRegionCode|\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|Czech          |Den české státnosti      |Den české státnosti      |null         |CZ               |\n|Norway         |Søndag                   |Søndag                   |null         |NO               |\n|Sweden         |Söndag                   |Söndag                   |null         |SE               |\n|India          |Gandhi Jayanti           |Gandhi Jayanti           |true         |IN               |\n|Germany        |Tag der Deutschen Einheit|Tag der Deutschen Einheit|null         |DE               |\n+---------------+-------------------------+-------------------------+-------------+-----------------+\nonly showing top 5 rows"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Read  the table we just created in the sql pool as a Spark dataframe\n",
        "val spark_read = spark.read.\n",
        "    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\n",
        "spark_read.show(5, truncate = false)"
      ],
      "attachments": {}
    }
  ],
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}